---
title: "Question C"
output: html_notebook
---

Per fornire uno score di fallimento si sono provati i seguenti modelli:
  - Logistic Regression
  - Elastic Net Regularization
  
Feature Selection:
  1) rimozione di attributi correlati tra loro (> di una certa soglia)
  2) procedura stepAIC per determinare il miglior sottoinsieme di attributi, 
   AIC è una stima della bontà del modello (considera il model fit e la semplicità del modello)

library
```{r}
#install.packages("caret", dep = TRUE)
library(skimr)
library(lattice)
library(data.table)
library(caret)
library(ROCR)
library(car)
```

```{r}
aida = fread("../aida_abs.csv")

# Make the operator
'%!in%' <- function(x,y)!('%in%'(x,y))

# Add new empty column
aida$Failed <- NULL

# For each 'Legal status' which contains 'Active' substring fill in new column 'No' else 'Yes'
act_vector <- c('Active','Active (default of payments)','Active (receivership)')
aida[aida$Legal.status %in% act_vector,'Failed'] <- 0
aida[aida$Legal.status %!in% act_vector,'Failed'] <- 1

aida[is.na(aida$Failed), ]
aida$Failed <- as.factor(aida$Failed)

drops <- c('Company.name','File','Tax.code.number','Legal.form','Legal.status','Comune.ISTAT.code','Region','Province')
aida <- subset(aida, select = -c(Company.name,File,ATECO.2007code,Legal.form,Tax.code.number,Legal.status,Comune.ISTAT.code,Region,Province))
```

Si considerano gli anni 2014 e 2015 per il training dei modelli dato che la distribuzione dell'attributo Failed è bilanciata
```{r}
table(na.omit(aida)$Failed, na.omit(aida)$Year)
data = aida[aida$Year %in% c(2014,2015)]
```


1) Un problema dei modelli lineari è l'alta correlazione tra le variabili indipendenti,
prima di costruire il regresore logistico si cerca di escludere le variabili con correlazione > di una certa soglia

  - In caso di correlazione tra due variabili, si elimina quella con VIF (Variance Inflation Factor) maggiore
  - In alternativa, il metodo findCorrelation della libreria caret, restituisce direttamente le variabili da escludere sulla base della mean absolute             correlation ("https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/findCorrelation"), oltre che le coppie di variabili che superano il         cutoff

```{r}
corr.matrix=cor(subset(na.omit(aida), select = -c(Failed)))
#corr.matrix[abs(corr.matrix) < 0.8] = NA
#View(corr.matrix)

to.remove = findCorrelation(corr.matrix, cutoff = .8, verbose = TRUE, names=TRUE)
to.remove
```

```{r}
colnames(data)
```

Coppie correlate individuate dal metodo findCorrelation, si prosegue scegliendo di elimanare quello con VIF maggiore
```{r}
cat(colnames(data)[19], "-", colnames(data)[21], "\n") #remove ROA
cat(colnames(data)[17], "-", colnames(data)[15], "\n") #remove Net.financial.positionth
cat(colnames(data)[4], "-", colnames(data)[18], "\n") #remove Cash.Flowth
cat(colnames(data)[15], "-", colnames(data)[16], "\n") #remove Net.financial.positionth
cat(colnames(data)[14], "-", colnames(data)[7]) #remove Current.ratio

```

Si guarda al valore di VIF, le variabili con valore alto devono essere rimosse

```{r}
noresampling = trainControl(method="none")

lr.fit = glm(Failed ~ ., data = na.omit(data), family=binomial("logit")) 

sort(vif(lr.fit))
```

Data Partitioning, si divide il dataset in training e test set (70%-30%)
la distribuzione di Failed nel training set è bilanciata
```{r}
set.seed(42)

trainIndex <- createDataPartition(data$Failed, p = .7, 
                                  list = FALSE, 
                                  times = 1)

aidaTrain <- data[ trainIndex,]
aidaTest  <- data[-trainIndex,]

aidaTrain <- na.omit(aidaTrain)
aidaTest <- na.omit(aidaTest)

aidaTest2013 = aida[aida$Year %in% c(2013)]
aidaTest2013 = na.omit(aidaTest2013)

#Si rimuove da subito il valori per eliminare il problema della multicollinearità tra le variabili
aidaTrain <- subset(aidaTrain, select = -c(Year, ROA, Net.financial.positionth, Cash.Flowth, Current.ratio))

table(aidaTrain$Failed)
table(na.omit(data)$Failed)
```

2) Variable selection usando la procedura stepAIC di Logistic regression a partire dal set di attributi ottenuto fino a questo momento

```{r}
lr.fit = train(Failed ~ ., data = aidaTrain, 
               method = "glmStepAIC", trControl=trainControl(method="none"),
               family=binomial(logit))

#Si salva la miglior formula
best.formula = as.formula(Failed ~ Incorporation.year + `Banks/turnover` + Cost.of.debit + `Current.liabilities/Tot.ass` + `Debt/equity.ratio`
               + `EBITDA/Vendite` + `Interest/Turnover` + Liquidity.ratio + Net.working.capitalth + Number.of.employees + `Profit.(loss)th` +
               ROE + ROI + ROS + `Total.assets.turnover.(times)`)

lr.fit
summary(lr.fit)
```


Valutazione delle performance sul test set (entrambi gli anni 2014 e 2015, 2013)
```{r}
lr.pred = predict(lr.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr.fit, newdata = aidaTest, type="prob") # predict_proba
lr.pconf = lr.prob[,2]

confusionMatrix(lr.pred, aidaTest$Failed, positive="1", mode = "prec_recall")

lr.pred2013 = predict(lr.fit, newdata = aidaTest2013, type="raw")
lr.prob2013 = predict(lr.fit, newdata = aidaTest2013, type="prob") 
lr.pconf2013 = lr.prob2013[,2]

confusionMatrix(lr.pred2013, aidaTest2013$Failed, positive="1", mode = "prec_recall")
```


Calibration plot
```{r}
cal_data2013 = calibration(aidaTest2013$Failed ~ lr.pconf2013, class="1")
cal_data = calibration(aidaTest$Failed ~ lr.pconf, class="1")

plot(cal_data$data$midpoint, cal_data$data$Percent,col="blue", type="o", xlab="Bin Midpoint", ylab="Observed Event Percentage", ylim=c(0,100))

points(cal_data2013$data$midpoint, cal_data2013$data$Percent, col="red", pch="*")
lines(cal_data2013$data$midpoint, cal_data2013$data$Percent, col="red")

#Perfettamente calibrato
abline(a=0, b=1, lwd=0.5, lty=2)

legend("bottomright", legend=c("2014+2015","2013", "Perfectly Calibrated"),
       col=c("blue","red", "black"),
        pch=c("o","*",""),lty=c(1,2,2))

#Plot separati
xyplot(cal_data2013)
xyplot(cal_data)

```


Si valutano alcune metriche al variare del cutoff, il cutoff dove si hanno valori migliori è circa 0.5 (quello di default)
```{r}
lr.prediction = prediction(lr.pconf, aidaTest$Failed)
lr.prediction2013 = prediction(lr.pconf2013, aidaTest2013$Failed)

lr.acc = performance(lr.prediction, "acc"); plot(lr.acc)

lr.tpr = performance(lr.prediction, "tpr"); plot(lr.tpr)

lr.f1 = performance(lr.prediction, "f"); plot(lr.f1)

lr.prec = performance(lr.prediction, "prec"); plot(lr.prec)
```
```{r}
#ROC Curve
lr.roc = performance(lr.prediction, "tpr", "fpr")
plot(lr.roc, colorize=T); abline(a=0, b= 1)

#AUC
o = performance(lr.prediction, "auc")
o@y.values[[1]]
```
```{r}
library(pROC)

par(pty="s")
roc(aidaTest2013$Failed, lr.pconf2013, plot=TRUE, legacy.axes=TRUE,
      xlab="False Positive Rate", ylab="True Positive Rate", print.auc=TRUE, col="red")
roc(aidaTest$Failed, lr.pconf, plot=TRUE, legacy.axes=TRUE, add=TRUE, col="blue",
      xlab="False Positive Rate", ylab="True Positive Rate", print.auc=TRUE, print.auc.y=0.45)

legend("bottomright", legend=c("2013", "2014+2015"),
       col=c("red","blue"), lwd=4)
par(pty="m")
```



```{r}
featurePlot(x=data.frame(lr.pconf), y=aidaTest$Failed, plot='density', auto.key = list(columns = 2))
```



Logistic Regression usando cross validation con custom summary function sullo stesso training set,
gli attributi usati sono quelli risultati dalla procedure glmStepAIC
```{r}
set.seed(42)

lev = c("class0", "class1")
aidaTrain$Failed = factor(aidaTrain$Failed); levels(aidaTrain$Failed) = lev
aidaTest$Failed = factor(aidaTest$Failed); levels(aidaTest$Failed) = lev

aidaTest2013$Failed = factor(aidaTest2013$Failed); levels(aidaTest2013$Failed) = lev

rcv = trainControl(method="repeatedcv",
                   repeats=5, number=10,
                   classProbs = TRUE,
                   savePredictions = TRUE, #per poi poter valutare le performance anche sul training set
                   summaryFunction = twoClassSummary) #ROC, Sens, Spec

lr2.fit = train(best.formula, data = aidaTrain, 
                method = "glm", trControl=rcv,
                metric="ROC", #ottimizza ROC
                family=binomial(logit))
lr2.fit
```
Dettagli su 5x10 fold di cv per il valore di AUC
```{r}
# details over 5x10 folds
lr2.folds = lr2.fit$resample[['ROC']]
mean(lr2.folds) # media dei folds
```


Intervallo di confidenza per l'AUC media
```{r}
library(boot)
mean.pos = function(d, pos) mean(d[pos])
b = boot(lr2.folds, mean.pos, R=1000)
plot(b)

# CI basato su bias correction and accelleration (best method)
bci = boot.ci(b, conf=0.95, type="bca")
bci
bci$bca[4:5]

bci = boot.ci(b, conf=0.95, type="basic")
bci
bci$basic[4:5]

# CI based on normal fitting
bci = boot.ci(b, conf=0.95, type="norm")
bci
bci$normal[2:3]
```


Performance sul test set
```{r}
lr.pred = predict(lr2.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr2.fit, newdata = aidaTest, type="prob") #predict_proba
lr.pconf = lr.prob[,2]

confusionMatrix(lr.pred, aidaTest$Failed, positive="class1", mode = "prec_recall")

lr.prediction = prediction(lr.pconf, aidaTest$Failed)

#lr.prediction = prediction(lr2.fit$pred$class1, lr2.fit$pred$obs) #Per valutare la ROC curve sui dati di training

lr.roc = performance(lr.prediction, "tpr", "fpr")
plot(lr.roc, colorize=T); abline(a=0, b= 1)
```

Si è deciso di creare un classificatore Random forest per poi fare un confronto con performance ottenute con logistic regression
Si fa il training sugli stessi dati per confrontare i folds delle rispettive cross validation
```{r}
set.seed(42)

rcv = trainControl(method="repeatedcv",
                   repeats=5, number=10,
                   classProbs = TRUE,
                   savePredictions = TRUE, #per poi poter valutare le performance anche sul training set
                   summaryFunction = twoClassSummary) #ROC, Sens, Spec

tunegrid = expand.grid(.mtry=sqrt(ncol(aidaTrain)))

rf.fit = train(Failed~., data = aidaTrain, 
               method = "rf", trControl=rcv,
               metric="ROC", tuneGrid=tunegrid,
               ntree=50)
rf.fit
```
Dettagli su 5x10 fold di cv per il valore di AUC
```{r}
rf.folds = rf.fit$resample[['ROC']]
mean(rf.folds) # media dei folds
```

```{r}
b = boot(rf.folds, mean.pos, R=1000)
plot(b)

# CI basato su bias correction and accelleration (best method)
bci = boot.ci(b, conf=0.95, type="bca")
bci
bci$bca[4:5]

bci = boot.ci(b, conf=0.95, type="basic")
bci
bci$basic[4:5]

# CI based on normal fitting
bci = boot.ci(b, conf=0.95, type="norm")
bci
bci$normal[2:3]
```


Performance sul test set
```{r}
rf.pred = predict(rf.fit, newdata = aidaTest, type="raw")
rf.prob = predict(rf.fit, newdata = aidaTest, type="prob")
rf.pconf = rf.prob[,2]

confusionMatrix(rf.pred, aidaTest$Failed, positive="class1", mode = "prec_recall")

rf.pred2013 = predict(rf.fit, newdata = aidaTest2013, type="raw")
rf.prob2013 = predict(rf.fit, newdata = aidaTest2013, type="prob") 
rf.pconf2013 = rf.prob2013[,2]

confusionMatrix(rf.pred2013, aidaTest2013$Failed, positive="class1", mode = "prec_recall")


rf.prediction = prediction(rf.pconf, aidaTest$Failed)
#lr.prediction = prediction(lr2.fit$pred$class1, lr2.fit$pred$obs) #Per valutare la ROC curve sui dati di training

rf.roc = performance(rf.prediction, "tpr", "fpr")
plot(lr.roc, colorize=T); abline(a=0, b= 1)
```

Confronto tra i valori di AUC dei folds di random forest e logistic regression
```{r}
library(BSDA)

plot(density(lr2.folds),xlim=c(0.5,0.7))
lines(density(rf.folds),col=2)

boxplot(lr2.folds, rf.folds,
        xlab="Modello",
        ylab="AUC",
        names=c("glm","rf")
        )
```

Test di normalità delle due distribuzioni con Shapiro test
In entrambi i casi non si può rifiutare l'ipotesi nulla e quindi si prosegue le analisi assumendo che le due distribuzioni siano normali
```{r}
shapiro.test(lr2.folds)
shapiro.test(rf.folds)
```

F-test per verificare se le due distribuzioni hanno la stessa varianza
Non si può rifiutare l'ipotesi nulla e assumendo che le distribuzioni abbiano la stessa varianza si procede con t-test (con varianza uguale)
```{r}
var(lr2.folds); var(rf.folds)
# F-test 
var.test(lr2.folds, rf.folds)
# t-test con assunzione di varianza uguale
t.test(lr2.folds, rf.folds, var.equal=T)
```

Wilcoxon test (distribuzioni con stessa forma ma con posizioni shifate)
```{r}
wilcox.test(lr2.folds, rf.folds)
```

Bootstrap t.test
```{r}
library(MKinfer)
boot.t.test(lr2.folds, rf.folds, R=1000)
```

Paired test, dovrebbe produrre i migliori risultati in quanto le misurazioni delle due distribuzioni si riferiscono agli stessi fold della cross validation
```{r}
t.test(lr2.folds, rf.folds, paired=TRUE)
#equivalente
t.test(lr2.folds - rf.folds, mu=0) 
```

Tutti i test effettutati sulle due distribuzioni evidenziano che l'ipotesi nulla deve essere rifiutata e che i valori medi di AUC per i due classificatori sono statisticamente differenti



Intervalli di confidenza per i coefficienti
```{r}
lr = glm(best.formula, data=aidaTrain, family=binomial("logit"))

confint(lr)
```

Statistical Test of Rating - Hosmer-Lçemeshow test

Per ogni bin (parametro g) di probability of default si confronta il numero di valori attesi con il numero dei valori osservati
```{r}
library(ResourceSelection)

hl = hoslem.test(lr.fit$finalModel$y, fitted(lr.fit$finalModel), g=11)
hl

cbind(hl$expected, hl$observed)
```
```{r}
hl = hoslem.test(lr$y, fitted(lr), g=21)
hl

cbind(hl$expected, hl$observed)
```



Infine, si osserva che "Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred" è un messaggio di warning che potrebbe essere causato dal problema della separazione perfetta.
Una possibile soluzione al problema è la regressione logistica penalizzata (elastic net)



