---
title: "Question C"
output: html_notebook
---

Per fornire uno score di fallimento si sono provati i seguenti modelli:
  - Logistic Regression
  - Penalized Logistic Regression
  - Elastic Net Regularization
  
Feature Selection:
  1) rimozione di attributi correlati tra loro (> di una certa soglia)
  2) procedura stepAIC per determinare il miglior sottoinsieme di attributi, 
   AIC è una stima della bontà del modello (considera il model fit e la semplicità del modello)

library
```{r}
#install.packages("caret", dep = TRUE)
library(skimr)
library(lattice)
library(ggplot2)
library(data.table)
library(caret)
library(ROCR)
library(glmnet)
```

```{r}
aida = fread("aida_preproc.csv")

# Make the operator
'%!in%' <- function(x,y)!('%in%'(x,y))

# Add new empty column
aida$Failed <- NULL

# For each 'Legal status' which contains 'Active' substring fill in new column 'No' else 'Yes'
act_vector <- c('Active','Active (default of payments)','Active (receivership)')
aida[aida$Legal.status %in% act_vector,'Failed'] <- 0
aida[aida$Legal.status %!in% act_vector,'Failed'] <- 1

aida[is.na(aida$Failed), ]
aida$Failed <- as.factor(aida$Failed)

drops <- c('Company.name','File','Tax.code.number','Legal.form','Legal.status','Comune.ISTAT.code','Region','Province')
aida <- subset(aida, select = -c(Company.name,ATECO.2007code,File,Tax.code.number,Legal.form,Legal.status,Comune.ISTAT.code,Region,Province))
```

```{r}
#create subsample of the dataset
sample = rbinom(nrow(aida), 1, .5)           
data = aida[sample==1,]

#data = aida[aida$Year==2016, ]

#dummies <- dummyVars( ~ ., data = data)
#head(predict(dummies, newdata = data))
```


1) Un problema dei modelli lineari è l'alta correlazione tra le variabili indipendenti,
prima di costruire il regresore logistico si cerca di escludere le variabili con correlazione > di una certa soglia

  - In caso di correlazione tra due variabili, si elimina quella con VIF (Variance Inflation Factor) maggiore
  - In alternativa, il metodo findCorrelation della libreria caret, restituisce direttamente le variabili da escludere sulla base della mean absolute             correlation ("https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/findCorrelation"), oltre che le coppie di variabili che superano il         cutoff

```{r}
corr.matrix=cor(subset(na.omit(aida), select = -c(Failed)))
#corr.matrix[abs(corr.matrix) < 0.8] = NA
#View(corr.matrix)
to.remove = findCorrelation(corr.matrix, cutoff = .8, verbose = TRUE, names=TRUE)
to.remove
```

```{r}
colnames(data)
```


```{r}
cat(colnames(data)[19], "-", colnames(data)[21], "\n") #ROS
cat(colnames(data)[17], "-", colnames(data)[15], "\n") #Net.working.capitalth
cat(colnames(data)[4], "-", colnames(data)[18], "\n") #ROA
cat(colnames(data)[15], "-", colnames(data)[16], "\n") #Net.working.capitalth
cat(colnames(data)[14], "-", colnames(data)[7]) #Net.financial.positionth

```

Si guarda al valore di VIF, le variabili con valore alto devono essere rimosse

```{r}
library(car)
noresampling = trainControl(method="none")

lr.fit = glm(Failed ~ ., data = na.omit(data), family=binomial("logit")) 

sort(vif(lr.fit))
```
Data Partitioning
```{r}
set.seed(42)
trainIndex <- createDataPartition(data$Failed, p = .7, 
                                  list = FALSE, 
                                  times = 1)

aidaTrain <- data[ trainIndex,]
aidaTest  <- data[-trainIndex,]

aidaTrain <- na.omit(aidaTrain)
aidaTest <- na.omit(aidaTest)

aidaTrain <- subset(aidaTrain, select = -c(Year))

table(aidaTrain$Failed)
```


```{r}
down.aidaTrain <- downSample(x = aidaTrain, y = aidaTrain$Failed)
down.aidaTrain <- subset(down.aidaTrain, select = -c(Class))
table(down.aidaTrain$Failed)  
```

```{r}
over.aidaTrain <- upSample(x = aidaTrain, y = aidaTrain$Failed)
over.aidaTrain <- subset(over.aidaTrain, select = -c(Class))
table(over.aidaTrain$Failed)      
```

Logistic regression eliminando le variabili altamente correlate
```{r}
# resampling method
noresampling = trainControl(method="none")

lr.fit = train(Failed ~ ., data = subset(aidaTrain, select=-c(ROS, Net.working.capitalth, ROA, Net.financial.positionth)), 
               method = "glm", trControl=noresampling,
               # pass-trough options
               family=binomial(logit))
lr.fit
summary(lr.fit)
```
```{r}
# predicted confidence on validation set
lr.pred = predict(lr.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr.fit, newdata = aidaTest, type="prob") # as predict_proba in scikit-learn
lr.pconf = lr.prob[,2]
# built-in metrics
confusionMatrix(lr.pred, aidaTest$Failed, positive="1", mode = "prec_recall")
```

2) Variable selection usando la procedura stepAIC su tre training set differenti (originale, bilanciato con undersampling, bilanciato con oversampling)

Logistic regression con trainingset originale (leggermente sbilanciato)

```{r}
# resampling method
noresampling = trainControl(method="none")

lr.fit = train(Failed ~ ., data = aidaTrain, 
               method = "glmStepAIC", trControl=noresampling,
               # pass-trough options
               family=binomial(logit))
lr.fit
summary(lr.fit)
```



```{r}
# predicted confidence on validation set
lr.pred = predict(lr.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr.fit, newdata = aidaTest, type="prob") # as predict_proba in scikit-learn
lr.pconf = lr.prob[,2]
# built-in metrics
confusionMatrix(lr.pred, aidaTest$Failed, positive="1", mode = "prec_recall")
```



```{r}
# calibration plot
cal_data = calibration(aidaTest$Failed ~ lr.pconf, class="1")
xyplot(cal_data)
```


Logistic regression con training set bilanciato con random undersampling

```{r}
# resampling method
noresampling = trainControl(method="none")

lr.fit = train(Failed ~ ., data = down.aidaTrain, 
               method = "glmStepAIC", trControl=noresampling,
               family=binomial(logit))
lr.fit
best.formula = formula(lr.fit)
summary(lr.fit)
```

```{r}
# predicted confidence on validation set
lr.pred = predict(lr.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr.fit, newdata = aidaTest, type="prob") # as predict_proba in scikit-learn
lr.pconf = lr.prob[,2]
# built-in metrics
confusionMatrix(lr.pred, aidaTest$Failed, positive="1", mode = "prec_recall")
```

```{r}
# using ROCR for metrics at cutoff
lr.prediction = prediction(lr.pconf, aidaTest$Failed)
# acc at cutoff
lr.acc = performance(lr.prediction, "acc"); plot(lr.acc)
# tpr (recall) at cutoff
lr.tpr = performance(lr.prediction, "tpr"); plot(lr.tpr)
# f1 at cutoff
lr.f1 = performance(lr.prediction, "f"); plot(lr.f1)
# precision at cutoff
lr.prec = performance(lr.prediction, "prec"); plot(lr.prec)
# roc curve
lr.roc = performance(lr.prediction, "tpr", "fpr")
plot(lr.roc, colorize=T); abline(a=0, b= 1)
```
```{r}
# AUC: performance returns an object whose members (called 'slots') are accessed with '@'
o = performance(lr.prediction, "auc")
o@y.values[[1]]
```

```{r}
featurePlot(x=data.frame(lr.pconf), y=aidaTest$Failed, plot='density', auto.key = list(columns = 2))
```

Cambio del decision threshold
```{r}
cutoff = 0.3
lr.pred.cutoff = factor(ifelse(lr.pconf>=cutoff, 1, 0), levels=levels(aidaTest$Failed))
confusionMatrix(lr.pred.cutoff, aidaTest$Failed, positive="1", mode = "prec_recall")
```


Logistic regression con training set bilanciato con random oversampling
```{r}
# resampling method
noresampling = trainControl(method="none")

lr.fit = train(Failed ~ ., data = over.aidaTrain, 
               method = "glmStepAIC", trControl=noresampling,
               # pass-trough options
               family=binomial(logit))
lr.fit
summary(lr.fit)
```

```{r}
# predicted confidence on validation set
lr.pred = predict(lr.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr.fit, newdata = aidaTest, type="prob") # as predict_proba in scikit-learn
lr.pconf = lr.prob[,2]
# built-in metrics
confusionMatrix(lr.pred, aidaTest$Failed, positive="1", mode = "prec_recall")
```

```{r}
# using ROCR for metrics at cutoff
lr.prediction = prediction(lr.pconf, aidaTest$Failed)
# acc at cutoff
lr.acc = performance(lr.prediction, "acc"); plot(lr.acc)
# tpr (recall) at cutoff
lr.tpr = performance(lr.prediction, "tpr"); plot(lr.tpr)
# f1 at cutoff
lr.f1 = performance(lr.prediction, "f"); plot(lr.f1)
# precision at cutoff
lr.prec = performance(lr.prediction, "prec"); plot(lr.prec)
# roc curve
lr.roc = performance(lr.prediction, "tpr", "fpr")
plot(lr.roc, colorize=T); abline(a=0, b= 1)
```
```{r}
# AUC: performance returns an object whose members (called 'slots') are accessed with '@'
o = performance(lr.prediction, "auc")
o@y.values[[1]]
```
```{r}
featurePlot(x=data.frame(lr.pconf), y=aidaTest$Failed, plot='density', auto.key = list(columns = 2))
```




Logistic Regression usando cross validation con custom summary function,
gli attributi usati sono quelli risultati dalla procedure glmStepAIC
il training set usato è quello su cui si è applicato random undersampling
```{r}

lev = c("class0", "class1")
down.aidaTrain$Failed = factor(down.aidaTrain$Failed); levels(down.aidaTrain$Failed) = lev
aidaTest$Failed = factor(aidaTest$Failed); levels(aidaTest$Failed) = lev

# repeated cross validation
rcv = trainControl(method="repeatedcv",
                   repeats=5, number=10,
                   classProbs = TRUE,
                   savePredictions = TRUE, #per poi poter valutare le performance anche sul training set
                   summaryFunction = twoClassSummary)

lr2.fit = train(best.formula, data = down.aidaTrain, 
                method = "glm", trControl=rcv,
                metric="ROC",
                family=binomial(logit))
lr2.fit
```

```{r}
# details over 5x10 folds
lr2.folds = lr2.fit$resample[['ROC']]
mean(lr2.folds) # reported is mean performance
```

```{r}
# predicted confidence on validation set
lr.pred = predict(lr2.fit, newdata = aidaTest, type="raw")
lr.prob = predict(lr2.fit, newdata = aidaTest, type="prob") # as predict_proba in scikit-learn
lr.pconf = lr.prob[,2]
# built-in metrics
confusionMatrix(lr.pred, aidaTest$Failed, positive="class1", mode = "prec_recall")

lr.prediction = prediction(lr.pconf, aidaTest$Failed)


#lr.prediction = prediction(lr2.fit$pred$class1, lr2.fit$pred$obs) #Per valutare la ROC curve sui dati di training

lr.roc = performance(lr.prediction, "tpr", "fpr")
plot(lr.roc, colorize=T); abline(a=0, b= 1)
```

Random forest per fare un confronto delle performance ottenute con logistic regression
Si fa il training sullo stesso training set per poi poter fare un confronto tra i folds delle rispettive cross validation
```{r}
set.seed(42)
tunegrid = expand.grid(.mtry=sqrt(ncol(down.aidaTrain))) 
rf.fit = train(Failed~., data = down.aidaTrain, 
               method = "rf", trControl=rcv,
               metric="ROC", tuneGrid=tunegrid,
               ntree=15)
rf.fit
```

```{r}
# details over 5x10 folds
rf.folds = rf.fit$resample[['ROC']]
mean(rf.folds) # reported is mean performance
```

Confronto tra i valori di AUC dei folds di random forest e logistic regression
```{r}
library(BSDA)

plot(density(lr2.folds),xlim=c(0.6,1))
lines(density(rf.folds),col=2)
```
Test di normalità delle due distribuzioni con Shapiro test
```{r}
shapiro.test(lr2.folds)
shapiro.test(rf.folds)
```

F-test per verificare se le due distribuzioni hanno la stessa varianza
Non si può rifiutare l'ipotesi nulla e assumendo che le distribuzioni abbiano la stessa varianza si procede con t-test
```{r}
var(lr2.folds); var(rf.folds)
# F-test 
var.test(lr2.folds, rf.folds)
t.test(lr2.folds, rf.folds, var.equal=T)
```

Wilcoxon test (distribuzioni con stessa forma ma con posizioni shifate)
```{r}
wilcox.test(lr2.folds, rf.folds)
```

Bootstrap t.test
```{r}
library(MKinfer)
boot.t.test(lr2.folds, rf.folds, R=1000)
```
Paired test, dovrebbe produrre i migliori risultati in quanto le misurazioni delle due distribuzioni si riferiscono agli stessi fold della cross validation
```{r}
t.test(lr2.folds, rf.folds, paired=TRUE) # or
t.test(lr2.folds - rf.folds, mu=0) 
```


