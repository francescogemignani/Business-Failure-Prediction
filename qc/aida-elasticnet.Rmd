---
title: "R Notebook"
output: html_notebook
---

Elastic Net Regularization using glmnet 
```{r}
aidaTrain <- na.omit(aidaTrain)
x <- aidaTrain[, 1:(ncol(aidaTrain)-1)]
y <- aidaTrain$Failed
elasticFit <- cv.glmnet(data.matrix(x), y, family = "binomial", type.measure = 'mse', nfolds = 5) #returns the best model using CV 
```

lambda.min is the value of lambda that gives the minimum mean cross-validated error
lambda.1se is the value of lambda that gives the most regularized model 
```{r}
plot(elasticFit)
```

Using LAMBDA.MIN (best)
```{r}
x.test <- aidaTest[, 1:(ncol(aidaTest)-1)]
y.test <- aidaTest$Failed

# predicted confidence on validation set
elas.pred = predict(elasticFit, newx = data.matrix(x.test), type="class", s = "lambda.min")
elas.prob = predict(elasticFit, newx = data.matrix(x.test), type="response", s = "lambda.min") #as predict proba
# built-in metrics
confusionMatrix(factor(elas.pred), factor(y.test), positive="1", mode = "prec_recall")

```

```{r}
# using ROCR for metrics at cutoff
library(ROCR)
elas.prediction = prediction(elas.prob, aidaTest$Failed)
# acc at cutoff
elas.acc = performance(elas.prediction, "acc"); plot(elas.acc, main = "Accuracy for lambda.min")
# tpr (recall) at cutoff
elas.tpr = performance(elas.prediction, "tpr"); plot(elas.tpr, main = "TPR for lambda.min")
# f1 at cutoff
elas.f1 = performance(elas.prediction, "f"); plot(elas.f1, main = "F1 for lambda.min")
# precision at cutoff
elas.prec = performance(elas.prediction, "prec"); plot(elas.prec, main = "Precision for lambda.min")
# roc curve
elas.roc = performance(elas.prediction, "tpr", "fpr")
plot(elas.roc, colorize=T, main = "AUC for lambda.min"); abline(a=0, b= 1)
cat("AUC value: ", performance(elas.prediction, "auc")@y.values[[1]])
```

Set cutoff to 0.2 (as we can see from the charts above)
```{r}
cutoff = 0.2
elas.pred.cutoff = factor(ifelse(elas.prob>=cutoff, 1, 0), levels=levels(aidaTest$Failed))
confusionMatrix(elas.pred.cutoff, aidaTest$Failed, positive="1", mode = "prec_recall")
```

Using LAMBDA.1SE (best)
```{r}
x.test <- aidaTest[, 1:(ncol(aidaTest)-1)]
y.test <- aidaTest$Failed

# predicted confidence on validation set
elas.pred.1se = predict(elasticFit, newx = data.matrix(x.test), type="class", s = "lambda.1se")
elas.prob.1se = predict(elasticFit, newx = data.matrix(x.test), type="response", s = "lambda.1se") #as predict proba
# built-in metrics
confusionMatrix(factor(elas.pred.1se), factor(y.test), positive="1", mode = "prec_recall")

```

```{r}
# using ROCR for metrics at cutoff
library(ROCR)
elas.prediction.1se = prediction(elas.prob.1se, aidaTest$Failed)
# acc at cutoff
elas.acc.1se = performance(elas.prediction.1se, "acc"); plot(elas.acc.1se, main = "Accuracy for lambda.1se")
# tpr (recall) at cutoff
elas.tpr.1se = performance(elas.prediction.1se, "tpr"); plot(elas.tpr.1se, main = "TPR for lambda.1se")
# f1 at cutoff
elas.f1.1se = performance(elas.prediction.1se, "f"); plot(elas.f1.1se, main = "F1 for lambda.1se")
# precision at cutoff
elas.prec.1se = performance(elas.prediction.1se, "prec"); plot(elas.prec.1se, main = "Precision for lambda.1se")
# roc curve
elas.roc.1se = performance(elas.prediction.1se, "tpr", "fpr")
plot(elas.roc.1se, colorize=T, main = "AUC for lambda.1se"); abline(a=0, b= 1)
cat("AUC value: ", performance(elas.prediction.1se, "auc")@y.values[[1]])
```

Set cutoff to 0.2 (as we can see from the charts above)
```{r}
cutoff = 0.2
elas.pred.cutoff.1se = factor(ifelse(elas.prob.1se>=cutoff, 1, 0), levels=levels(aidaTest$Failed))
confusionMatrix(elas.pred.cutoff.1se, aidaTest$Failed, positive="1", mode = "prec_recall")
```

