---
title: "Question C, using elasticnet"
output: html_notebook
---

```{r} 
library
#install.packages("caret", dep = TRUE)
library(lattice)
library(ggplot2)
library(data.table)
library(caret)
library(glmnet)
library(MASS)
library(glasso)
library(sAIC)
library(car)
```

```{r}
aida = fread("../aida_abs.csv")

# Make the operator
'%!in%' <- function(x,y)!('%in%'(x,y))

# Add new empty column
aida$Failed <- NULL

# For each 'Legal status' which contains 'Active' substring fill in new column 'No' else 'Yes'
act_vector <- c('Active','Active (default of payments)','Active (receivership)')
aida[aida$Legal.status %in% act_vector,'Failed'] <- 0
aida[aida$Legal.status %!in% act_vector,'Failed'] <- 1

aida[is.na(aida$Failed), ]
aida$Failed <- as.factor(aida$Failed)

aida <- na.omit(aida)
drops <- c('Company.name','File','Tax.code.number','Legal.form','Legal.status','Comune.ISTAT.code','Region','Province')
aida <- subset(aida, select = -c(Company.name,ATECO.2007code,File,Tax.code.number,Legal.form,Legal.status,Comune.ISTAT.code,Region,Province))
```

Get 2014 and 2015 data because the distribution considering Failed is balanced 
```{r}
data = na.omit(aida[aida$Year %in% c(2014,2015)])
table(data$Failed)
```

1) Un problema dei modelli lineari Ã¨ l'alta correlazione tra le variabili indipendenti,
prima di costruire il regresore logistico si cerca di escludere le variabili con correlazione > di una certa soglia

  - In caso di correlazione tra due variabili, si elimina quella con VIF (Variance Inflation Factor) maggiore
  - In alternativa, il metodo findCorrelation della libreria caret, restituisce direttamente le variabili da escludere sulla base della mean absolute             correlation ("https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/findCorrelation"), oltre che le coppie di variabili che superano la        soglia prestabilita

```{r}
corr.matrix=cor(subset(data, select = -c(Failed)))
#corr.matrix[abs(corr.matrix) < 0.8] = NA
#View(corr.matrix)
to.remove = findCorrelation(corr.matrix, cutoff = .8, verbose = TRUE, names=TRUE)
to.remove
#data = subset(data, select = -c(Cash.Flowth, EBITDAth, Total.assetsth, ROA, Liquidity.ratio))
```

```{r}
cat(colnames(data)[17], "-", colnames(data)[25], "\n") #remove Total.assetsth
cat(colnames(data)[25], "-", colnames(data)[4], "\n") #remove Total.assetsth
cat(colnames(data)[4], "-", colnames(data)[15], "\n") #remove Net.financial.positionth
cat(colnames(data)[15], "-", colnames(data)[18], "\n") #remove Net.financial.positionth
cat(colnames(data)[18], "-", colnames(data)[16], "\n") #remove Net.working.capitalth 
cat(colnames(data)[19], "-", colnames(data)[21], "\n") #remove ROA
cat(colnames(data)[14], "-", colnames(data)[7], "\n") #remove Current.ratio
```

Si guarda al valore di VIF, le variabili con valore alto devono essere rimosse

```{r}
noresampling = trainControl(method="none")

lr.fit = glm(Failed ~ ., data = na.omit(data), family=binomial("logit")) 

sort(vif(lr.fit))
```

Remove useless columns
```{r}
data <- subset(data, select = -c(Total.assetsth, Net.financial.positionth, Net.working.capitalth, ROA, Current.ratio, Year))
```

Partitioning dataset 
```{r}
set.seed(42)
trainIndex <- createDataPartition(data$Failed, p = .7, 
                                  list = FALSE, 
                                  times = 1)

aidaTrain <- data[ trainIndex,]
aidaTest  <- data[-trainIndex,]

aidaTest2013 = aida[aida$Year %in% c(2013)]
aidaTest2013 = na.omit(aidaTest2013)
aidaTest2013 <- subset(aidaTest2013, select = -c(Total.assetsth, Net.financial.positionth, Net.working.capitalth, ROA, Current.ratio, Year))

table(aidaTrain$Failed)
```

Elastic Net Regularization using glmnet 
```{r}
x <- aidaTrain[, 1:(ncol(aidaTrain)-1)]
y <- aidaTrain$Failed
elasticFit <- cv.glmnet(data.matrix(x), y, family = "binomial", type.measure = 'mse', nfolds = 5, resample.seed = NULL) #returns the best model using CV 
```

```{r}
coef(elasticFit, s = "lambda.min")
```


lambda.min is the value of lambda that gives the minimum mean cross-validated error
lambda.1se is the value of lambda that gives the most regularized model 
```{r}
plot(elasticFit)
```

Using LAMBDA.MIN (best)
```{r}
x.test <- aidaTest[, 1:(ncol(aidaTest)-1)]
y.test <- aidaTest$Failed

# predicted confidence on validation set
elas.pred = predict(elasticFit, newx = data.matrix(x.test), type="class", s = "lambda.min")
elas.prob = predict(elasticFit, newx = data.matrix(x.test), type="response", s = "lambda.min") #as predict proba
# built-in metrics
confusionMatrix(factor(elas.pred), factor(y.test), positive="1", mode = "prec_recall")

```

0,5 is the best cutoff to use, so we're not modifying it
```{r}
# using ROCR for metrics at cutoff
library(ROCR)
elas.prediction = prediction(elas.prob, aidaTest$Failed)
# acc at cutoff
elas.acc = performance(elas.prediction, "acc"); plot(elas.acc, main = "Accuracy for lambda.min")
# tpr (recall) at cutoff
elas.tpr = performance(elas.prediction, "tpr"); plot(elas.tpr, main = "TPR for lambda.min")
# f1 at cutoff
elas.f1 = performance(elas.prediction, "f"); plot(elas.f1, main = "F1 for lambda.min")
# precision at cutoff
elas.prec = performance(elas.prediction, "prec"); plot(elas.prec, main = "Precision for lambda.min")
# roc curve
elas.roc = performance(elas.prediction, "tpr", "fpr")
plot(elas.roc, colorize=T, main = "AUC for lambda.min"); abline(a=0, b= 1)
cat("AUC value: ", performance(elas.prediction, "auc")@y.values[[1]])
```


Using LAMBDA.1SE 
```{r}
# predicted confidence on validation set
elas.pred.1se = predict(elasticFit, newx = data.matrix(x.test), type="class", s = "lambda.1se")
elas.prob.1se = predict(elasticFit, newx = data.matrix(x.test), type="response", s = "lambda.1se") #as predict proba
# built-in metrics
confusionMatrix(factor(elas.pred.1se), factor(y.test), positive="1", mode = "prec_recall")

```

Best cutoff still is 0,5
```{r}
# using ROCR for metrics at cutoff
library(ROCR)
elas.prediction.1se = prediction(elas.prob.1se, aidaTest$Failed)
# acc at cutoff
elas.acc.1se = performance(elas.prediction.1se, "acc"); plot(elas.acc.1se, main = "Accuracy for lambda.1se")
# tpr (recall) at cutoff
elas.tpr.1se = performance(elas.prediction.1se, "tpr"); plot(elas.tpr.1se, main = "TPR for lambda.1se")
# f1 at cutoff
elas.f1.1se = performance(elas.prediction.1se, "f"); plot(elas.f1.1se, main = "F1 for lambda.1se")
# precision at cutoff
elas.prec.1se = performance(elas.prediction.1se, "prec"); plot(elas.prec.1se, main = "Precision for lambda.1se")
# roc curve
elas.roc.1se = performance(elas.prediction.1se, "tpr", "fpr")
plot(elas.roc.1se, colorize=T, main = "AUC for lambda.1se"); abline(a=0, b= 1)
cat("AUC value: ", performance(elas.prediction.1se, "auc")@y.values[[1]])
```


Predict whether a company failed on 2013, using the model trained on the years 2014-2015 (using lambda.min)
```{r}

x.2013.test <- aidaTest2013[, 1:(ncol(aidaTest2013)-1)]
y.2013.test <- aidaTest2013$Failed

# predicted confidence on validation set
elas.2013.pred = predict(elasticFit, newx = data.matrix(x.2013.test), type="class", s = "lambda.min")
elas.2013.prob = predict(elasticFit, newx = data.matrix(x.2013.test), type="response", s = "lambda.min") #as predict proba
# built-in metrics
confusionMatrix(factor(elas.2013.pred), factor(y.2013.test), positive="1", mode = "prec_recall")
```

Calibration plot
```{r}
cal_data2013 = calibration(aidaTest2013$Failed ~ elas.2013.prob, class="1")
cal_data = calibration(aidaTest$Failed ~ elas.prob, class="1")

plot(cal_data$data$midpoint, cal_data$data$Percent,col="blue", type="o", xlab="Bin Midpoint", ylab="Observed Event Percentage", ylim=c(0,100))

points(cal_data2013$data$midpoint, cal_data2013$data$Percent, col="red", pch="*")
lines(cal_data2013$data$midpoint, cal_data2013$data$Percent, col="red")

#Perfettamente calibrato
abline(a=0, b=1, lwd=0.5, lty=2)

legend("bottomright", legend=c("2014+2015","2013", "Perfectly Calibrated"),
       col=c("blue","red", "black"),
        pch=c("o","*",""),lty=c(1,2,2))

#Plot separati
xyplot(cal_data2013)
xyplot(cal_data)

```

```{r}
# using ROCR for metrics at cutoff
library(ROCR)
elas.2013.prediction = prediction(elas.2013.prob, aidaTest2013$Failed)
# acc at cutoff
elas.2013.acc = performance(elas.2013.prediction, "acc"); plot(elas.2013.acc, main = "Accuracy on 2013 test data for lambda.min")
# tpr (recall) at cutoff
elas.2013.tpr = performance(elas.2013.prediction, "tpr"); plot(elas.2013.tpr, main = "TPR on 2013 test data for lambda.min")
# f1 at cutoff
elas.2013.f1 = performance(elas.2013.prediction, "f"); plot(elas.2013.f1, main = "F1 on 2013 test data for lambda.min")
# precision at cutoff
elas.2013.prec = performance(elas.2013.prediction, "prec"); plot(elas.2013.prec, main = "Precision on 2013 test data for lambda.min")
# roc curve
elas.2013.roc = performance(elas.2013.prediction, "tpr", "fpr")
plot(elas.2013.roc, colorize=T, main = "AUC for lambda.min"); abline(a=0, b= 1)
cat("AUC value: ", performance(elas.2013.prediction, "auc")@y.values[[1]])
```


See performances for lambda.1se
```{r}

x.2013.test <- aidaTest2013[, 1:(ncol(aidaTest2013)-1)]
y.2013.test <- aidaTest2013$Failed

# predicted confidence on validation set
elas.2013.pred.1se = predict(elasticFit, newx = data.matrix(x.2013.test), type="class", s = "lambda.1se")
elas.2013.prob.1se = predict(elasticFit, newx = data.matrix(x.2013.test), type="response", s = "lambda.1se") #as predict proba
# built-in metrics
confusionMatrix(factor(elas.2013.pred), factor(y.2013.test), positive="1", mode = "prec_recall")
```

```{r}
# using ROCR for metrics at cutoff
library(ROCR)
elas.2013.prediction = prediction(elas.2013.prob.1se, aidaTest2013$Failed)
# acc at cutoff
elas.2013.acc = performance(elas.2013.prediction, "acc"); plot(elas.2013.acc, main = "Accuracy on 2013 test data for lambda.1se")
# tpr (recall) at cutoff
elas.2013.tpr = performance(elas.2013.prediction, "tpr"); plot(elas.2013.tpr, main = "TPR on 2013 test data for lambda.1se")
# f1 at cutoff
elas.2013.f1 = performance(elas.2013.prediction, "f"); plot(elas.2013.f1, main = "F1 on 2013 test data for lambda.1se")
# precision at cutoff
elas.2013.prec = performance(elas.2013.prediction, "prec"); plot(elas.2013.prec, main = "Precision on 2013 test data for lambda.1se")
# roc curve
elas.2013.roc = performance(elas.2013.prediction, "tpr", "fpr")
plot(elas.2013.roc, colorize=T, main = "AUC for lambda.1se"); abline(a=0, b= 1)
cat("AUC value: ", performance(elas.2013.prediction, "auc")@y.values[[1]])
```


```{r}
library(pROC)

par(pty="s")
roc(aidaTest2013$Failed, elas.2013.prob.1se, plot=TRUE, legacy.axes=TRUE,
      xlab="False Positive Rate", ylab="True Positive Rate", print.auc=TRUE, col="red")
roc(aidaTest$Failed, elas.prob.1se, plot=TRUE, legacy.axes=TRUE, add=TRUE, col="blue",
      xlab="False Positive Rate", ylab="True Positive Rate", print.auc=TRUE, print.auc.y=0.45)

legend("bottomright", legend=c("2013", "2014+2015"),
       col=c("red","blue"), lwd=4)
par(pty="m")
```

